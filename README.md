
# ğŸ› ï¸ Proyecto ETL con EvaluaciÃ³n de Calidad de Datos

Este proyecto implementa un pipeline **ETL (Extract, Transform, Load)** para los datasets `customers` y `retail`, con el objetivo de integrar, limpiar y validar datos para la construcciÃ³n de una tabla de hechos `fact_sales.csv`.  
AdemÃ¡s, incluye una evaluaciÃ³n de calidad y la generaciÃ³n de **KPIs y visualizaciones** para apoyar los objetivos de negocio.

---

## ğŸ“‚ Estructura del Proyecto

```

ETL/
â”‚â”€â”€ data/                       # Archivos de datos
â”‚   â”œâ”€â”€ customer_data.csv
â”‚   â”œâ”€â”€ retail_data.csv
â”‚   â”œâ”€â”€ fact_sales.csv          # Tabla de hechos final
â”‚
â”‚â”€â”€ extract.py                   # MÃ³dulo de extracciÃ³n de datos
â”‚â”€â”€ transform.py                 # MÃ³dulo de limpieza y transformaciones
â”‚â”€â”€ main.py                      # Script principal ETL
â”‚â”€â”€ KPI's.ipynb                  # CÃ¡lculo de KPIs y visualizaciones
â”‚â”€â”€ qualityEvaluation.ipynb      # EvaluaciÃ³n de calidad de datos
â”‚â”€â”€ EDA.ipynb                    # AnÃ¡lisis exploratorio inicial
â”‚â”€â”€ EDA_Customers.ipynb          # AnÃ¡lisis exploratorio Customers
â”‚â”€â”€ EDA_Retail.ipynb             # AnÃ¡lisis exploratorio Retail
â”‚â”€â”€ requirements.txt             # Dependencias del proyecto
â”‚â”€â”€ README.md                    # DocumentaciÃ³n del proyecto

````

---

## âš™ï¸ Proceso ETL

### 1. **Extract**
- Se cargan los datasets `customer_data.csv` y `retail_data.csv`.

### 2. **Transform**
- Limpieza de datos mediante funciones especÃ­ficas en `transform.py`:
  - **TelÃ©fonos** estandarizados en formato `+1 (XXX) XXX-XXXX`.
  - **Correos electrÃ³nicos** en minÃºsculas y validados con regex.
  - **Fechas** convertidas a formato estÃ¡ndar `YYYY-MM-DD`.
  - **Edad** restringida al rango 13â€“120 aÃ±os.
  - **Montos** (`amount`) numÃ©ricos y â‰¥ 0.
  - **CategorÃ­as** validadas contra un catÃ¡logo maestro.
  - **Duplicados**:
    - Registros idÃ©nticos â†’ eliminados.
    - IDs repetidos con variaciones â†’ se reasigna un identificador Ãºnico (`1`, `2`, â€¦).
  - Valores invÃ¡lidos o faltantes â†’ reemplazados por `"No especificado"`.

### 3. **Load**
- Se construye la tabla de hechos `fact_sales.csv`, integrando clientes y transacciones mediante `customer_id`.

---

## ğŸ“Š EvaluaciÃ³n de Calidad
En `qualityEvaluation.ipynb` se aplicaron polÃ­ticas de calidad:
- **Completitud**: todas las columnas crÃ­ticas alcanzan 100% (con `"No especificado"`).
- **Exactitud**: validaciones de formato y rango.
- **Consistencia**: estandarizaciÃ³n de fechas y categorÃ­as.
- **Unicidad**: eliminaciÃ³n/correcciÃ³n de duplicados en `customer_id` y `transaction_id`.
- **Validez**: detecciÃ³n de valores fuera de dominio.

---

## ğŸ“ˆ KPIs y Visualizaciones

### Integridad Financiera
- Total de ventas, promedio por transacciÃ³n, ventas por vendedor.
- Visualizaciones: barras y distribuciones.

### PlaneaciÃ³n EstratÃ©gica
- Ventas mensuales, nÃºmero de transacciones, ticket promedio mensual.
- Visualizaciones: lÃ­neas de tiempo.

### Conocimiento de Clientes y Productos
- Ventas por categorÃ­a de producto.
- Top 10 clientes.
- DistribuciÃ³n de ventas por gÃ©nero y rango de edad.
- Visualizaciones: barras, donas y histogramas.

### Transparencia en Reportes
- % de completitud por columna.
- Clientes sin contacto.
- Visualizaciones: barras y pie charts.

---

## ğŸ”§ Requerimientos

Instalar dependencias con:

```bash
pip install -r requirements.txt
````

Principales librerÃ­as:

* `pandas`
* `numpy`
* `matplotlib`
* `seaborn`

---

## â–¶ï¸ EjecuciÃ³n

1. Colocar los datasets iniciales en la carpeta `data/`.
2. Ejecutar el pipeline ETL:

```bash
python main.py
```

3. Revisar la tabla final en:

```
data/fact_sales.csv
```

4. Analizar KPIs y calidad en los notebooks `KPI's.ipynb` y `qualityEvaluation.ipynb`.

---

